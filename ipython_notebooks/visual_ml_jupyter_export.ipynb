{"nbformat": 4, "nbformat_minor": 0, "cells": [{"source": ["# Predicting High Revenue in customer_data_clean_python"], "cell_type": "markdown", "metadata": {}}, {"source": ["### Notebook automatically generated from your model"], "cell_type": "markdown", "metadata": {}}, {"source": ["Model Random forest (s4), trained on 2022-04-05 07:58:09."], "cell_type": "markdown", "metadata": {}}, {"source": ["#### Generated on 2023-03-06 16:30:18.452473"], "cell_type": "markdown", "metadata": {}}, {"source": ["prediction\nThis notebook will reproduce the steps for a BINARY_CLASSIFICATION on  customer_data_clean_python.\nThe main objective is to predict the variable High Revenue"], "cell_type": "markdown", "metadata": {}}, {"source": ["#### Warning"], "cell_type": "markdown", "metadata": {}}, {"source": ["The goal of this notebook is to provide an easily readable and explainable code that reproduces the main steps\nof training the model. It is not complete: some of the preprocessing done by the DSS visual machine learning is not\nreplicated in this notebook. This notebook will not give the same results and model performance as the DSS visual machine\nlearning model."], "cell_type": "markdown", "metadata": {}}, {"source": ["Let's start with importing the required libs :"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["import sys\nimport dataiku\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport dataiku.core.pandasutils as pdu\nfrom dataiku.doctor.preprocessing import PCA\nfrom collections import defaultdict, Counter"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["And tune pandas display options:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["pd.set_option('display.width', 3000)\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["#### Importing base data"], "cell_type": "markdown", "metadata": {}}, {"source": ["The first step is to get our machine learning dataset:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["# We apply the preparation that you defined. You should not modify this.\npreparation_steps = []\npreparation_output_schema = {u'userModified': False, u'columns': [{u'type': u'string', u'name': u'customer_id'}, {u'type': u'string', u'name': u'ip'}, {u'type': u'string', u'name': u'ip_geopoint'}, {u'type': u'string', u'name': u'Country'}, {u'type': u'double', u'name': u'pages_visited'}, {u'type': u'boolean', u'name': u'campaign'}, {u'type': u'bigint', u'name': u'GDP_per_cap'}, {u'type': u'bigint', u'name': u'age'}, {u'type': u'double', u'name': u'price_first_item_purchased'}, {u'type': u'string', u'name': u'gender'}, {u'type': u'bigint', u'name': u'revenue'}, {u'type': u'string', u'name': u'Tier_Rev'}, {u'type': u'boolean', u'name': u'High Revenue'}]}\n\nml_dataset_handle = dataiku.Dataset('customer_data_clean_python')\nml_dataset_handle.set_preparation_steps(preparation_steps, preparation_output_schema)\n%time ml_dataset = ml_dataset_handle.get_dataframe(limit = 100000)\n\nprint ('Base data has %i rows and %i columns' % (ml_dataset.shape[0], ml_dataset.shape[1]))\n# Five first records\",\nml_dataset.head(5)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["#### Initial data management"], "cell_type": "markdown", "metadata": {}}, {"source": ["The preprocessing aims at making the dataset compatible with modeling.\nAt the end of this step, we will have a matrix of float numbers, with no missing values.\nWe'll use the features and the preprocessing steps defined in Models.\n\nLet's only keep selected features"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["ml_dataset = ml_dataset[[u'pages_visited', u'GDP_per_cap', u'campaign', u'Country', u'gender', u'High Revenue', u'age', u'price_first_item_purchased']]"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Let's first coerce categorical columns into unicode, numerical features into floats."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["# astype('unicode') does not work as expected\n\ndef coerce_to_unicode(x):\n    if sys.version_info < (3, 0):\n        if isinstance(x, str):\n            return unicode(x,'utf-8')\n        else:\n            return unicode(x)\n    else:\n        return str(x)\n\n\ncategorical_features = [u'campaign', u'Country', u'gender']\nnumerical_features = [u'pages_visited', u'GDP_per_cap', u'age', u'price_first_item_purchased']\ntext_features = []\nfrom dataiku.doctor.utils import datetime_to_epoch\nfor feature in categorical_features:\n    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)\nfor feature in text_features:\n    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)\nfor feature in numerical_features:\n    if ml_dataset[feature].dtype == np.dtype('M8[ns]') or (hasattr(ml_dataset[feature].dtype, 'base') and ml_dataset[feature].dtype.base == np.dtype('M8[ns]')):\n        ml_dataset[feature] = datetime_to_epoch(ml_dataset[feature])\n    else:\n        ml_dataset[feature] = ml_dataset[feature].astype('double')"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We are now going to handle the target variable and store it in a new variable:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["target_map = {u'true': 1, u'false': 0}\nml_dataset['__target__'] = ml_dataset['High Revenue'].map(str).map(target_map)\ndel ml_dataset['High Revenue']\n\n\n# Remove rows for which the target is unknown.\nml_dataset = ml_dataset[~ml_dataset['__target__'].isnull()]\n\nml_dataset['__target__'] = ml_dataset['__target__'].astype(np.int64)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["#### Cross-validation strategy"], "cell_type": "markdown", "metadata": {}}, {"source": ["The dataset needs to be split into 2 new sets, one that will be used for training the model (train set)\nand another that will be used to test its generalization capability (test set)"], "cell_type": "markdown", "metadata": {}}, {"source": ["This is a simple cross-validation strategy."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["train, test = pdu.split_train_valid(ml_dataset, prop=0.8)\nprint ('Train data has %i rows and %i columns' % (train.shape[0], train.shape[1]))\nprint ('Test data has %i rows and %i columns' % (test.shape[0], test.shape[1]))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["#### Features preprocessing"], "cell_type": "markdown", "metadata": {}}, {"source": ["The first thing to do at the features level is to handle the missing values.\nLet's reuse the settings defined in the model"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["drop_rows_when_missing = []\nimpute_when_missing = [{'impute_with': u'MEAN', 'feature': u'pages_visited'}, {'impute_with': u'MEAN', 'feature': u'GDP_per_cap'}, {'impute_with': u'MEAN', 'feature': u'age'}, {'impute_with': u'MEAN', 'feature': u'price_first_item_purchased'}]\n\n# Features for which we drop rows with missing values\"\nfor feature in drop_rows_when_missing:\n    train = train[train[feature].notnull()]\n    test = test[test[feature].notnull()]\n    print ('Dropped missing records in %s' % feature)\n\n# Features for which we impute missing values\"\nfor feature in impute_when_missing:\n    if feature['impute_with'] == 'MEAN':\n        v = train[feature['feature']].mean()\n    elif feature['impute_with'] == 'MEDIAN':\n        v = train[feature['feature']].median()\n    elif feature['impute_with'] == 'CREATE_CATEGORY':\n        v = 'NULL_CATEGORY'\n    elif feature['impute_with'] == 'MODE':\n        v = train[feature['feature']].value_counts().index[0]\n    elif feature['impute_with'] == 'CONSTANT':\n        v = feature['value']\n    train[feature['feature']] = train[feature['feature']].fillna(v)\n    test[feature['feature']] = test[feature['feature']].fillna(v)\n    print ('Imputed missing values in feature %s with value %s' % (feature['feature'], coerce_to_unicode(v)))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We can now handle the categorical features (still using the settings defined in Models):"], "cell_type": "markdown", "metadata": {}}, {"source": ["Let's dummy-encode the following features.\nA binary column is created for each of the 100 most frequent values."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["LIMIT_DUMMIES = 100\n\ncategorical_to_dummy_encode = [u'campaign', u'Country', u'gender']\n\n# Only keep the top 100 values\ndef select_dummy_values(train, features):\n    dummy_values = {}\n    for feature in categorical_to_dummy_encode:\n        values = [\n            value\n            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n        ]\n        dummy_values[feature] = values\n    return dummy_values\n\nDUMMY_VALUES = select_dummy_values(train, categorical_to_dummy_encode)\n\ndef dummy_encode_dataframe(df):\n    for (feature, dummy_values) in DUMMY_VALUES.items():\n        for dummy_value in dummy_values:\n            dummy_name = u'%s_value_%s' % (feature, coerce_to_unicode(dummy_value))\n            df[dummy_name] = (df[feature] == dummy_value).astype(float)\n        del df[feature]\n        print ('Dummy-encoded feature %s' % feature)\n\ndummy_encode_dataframe(train)\n\ndummy_encode_dataframe(test)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Let's rescale numerical features"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["rescale_features = {u'age': u'AVGSTD', u'pages_visited': u'AVGSTD', u'GDP_per_cap': u'AVGSTD', u'price_first_item_purchased': u'AVGSTD'}\nfor (feature_name, rescale_method) in rescale_features.items():\n    if rescale_method == 'MINMAX':\n        _min = train[feature_name].min()\n        _max = train[feature_name].max()\n        scale = _max - _min\n        shift = _min\n    else:\n        shift = train[feature_name].mean()\n        scale = train[feature_name].std()\n    if scale == 0.:\n        del train[feature_name]\n        del test[feature_name]\n        print ('Feature %s was dropped because it has no variance' % feature_name)\n    else:\n        print ('Rescaled %s' % feature_name)\n        train[feature_name] = (train[feature_name] - shift).astype(np.float64) / scale\n        test[feature_name] = (test[feature_name] - shift).astype(np.float64) / scale"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["#### Modeling"], "cell_type": "markdown", "metadata": {}}, {"source": ["Before actually creating our model, we need to split the datasets into their features and labels parts:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["X_train = train.drop('__target__', axis=1)\nX_test = test.drop('__target__', axis=1)\n\ny_train = np.array(train['__target__'])\ny_test = np.array(test['__target__'])"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Now we can finally create our model!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100,\n    random_state=1337,\n    max_depth=13,\n    min_samples_leaf=1,\n    verbose=2)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We set \"class_weight\" as the weighting strategy:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["clf.class_weight = \"balanced\""], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["... And train the model"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["%time clf.fit(X_train, y_train)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Build up our result dataset"], "cell_type": "markdown", "metadata": {}}, {"source": ["The model is now trained, we can apply it to our test set:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["%time _predictions = clf.predict(X_test)\n%time _probas = clf.predict_proba(X_test)\npredictions = pd.Series(data=_predictions, index=X_test.index, name='predicted_value')\ncols = [\n    u'probability_of_value_%s' % label\n    for (_, label) in sorted([(int(target_map[label]), label) for label in target_map])\n]\nprobabilities = pd.DataFrame(data=_probas, index=X_test.index, columns=cols)\n\n# Build scored dataset\nresults_test = X_test.join(predictions, how='left')\nresults_test = results_test.join(probabilities, how='left')\nresults_test = results_test.join(test['__target__'], how='left')\nresults_test = results_test.rename(columns= {'__target__': 'High Revenue'})"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Let's have a look at feature importances"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["feature_importances_data = []\nfeatures = X_train.columns\nfor feature_name, feature_importance in zip(features, clf.feature_importances_):\n    feature_importances_data.append({\n        'feature': feature_name,\n        'importance': feature_importance\n    })\n\n# Plot the results\npd.DataFrame(feature_importances_data)\\\n    .set_index('feature')\\\n    .sort_values(by='importance')[-10::]\\\n    .plot(title='Top 10 most important variables',\n          kind='barh',\n          figsize=(10, 6),\n          color='#348ABD',\n          alpha=0.6,\n          lw='1',\n          edgecolor='#348ABD',\n          grid=False,)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["#### Results"], "cell_type": "markdown", "metadata": {}}, {"source": ["You can measure the model's accuracy:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["from dataiku.doctor.utils.metrics import mroc_auc_score\ny_test_ser = pd.Series(y_test)\n \nprint ('AUC value:', mroc_auc_score(y_test_ser, _probas))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We can also view the predictions directly.\nSince scikit-learn only predicts numericals, the labels have been mapped to 0,1,2 ...\nWe need to 'reverse' the mapping to display the initial labels."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 0, "cell_type": "code", "source": ["inv_map = { target_map[label] : label for label in target_map}\npredictions.map(inv_map)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["That's it. It's now up to you to tune your preprocessing, your algo, and your analysis !\n"], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "codemirror_mode": {"version": 2, "name": "ipython"}}, "name": "Predicting High Revenue in customer_data_clean_python"}}