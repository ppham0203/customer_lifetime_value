{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-mlflow",
      "display_name": "Python (env mlflow)",
      "language": "python"
    },
    "associatedRecipe": "recipe_from_notebook_clv_train",
    "creator": "chris.helmus@dataiku.com",
    "createdOn": 1678988331552,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "chris.helmus@dataiku.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd\nimport os\nimport warnings\nimport sys\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom urllib.parse import urlparse\nimport mlflow\nimport mlflow.sklearn\nfrom datetime import datetime\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom dataikuapi.dss.ml import DSSPredictionMLTaskSettings"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#setup metadata\nXP_TRACKING_FOLDER_ID \u003d \"CacxT6xM\"\nMLFLOW_EXPERIMENT_NAME \u003d \"clv-mlflow-exp\"\nMLFLOW_CODE_ENV_NAME \u003d \"mlflow_py36\"\nSAVED_MODEL_NAME \u003d \"clv-classifier-mlflow\"\nEVALUATION_DATASET \u003d \"customer_data_test_prepared\"\nMODEL_NAME \u003d \"catboost\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#read inputs\ntrain \u003d dataiku.Dataset(\"customer_data_train_prepared\")\ntrain_df \u003d train.get_dataframe()\n#train_df \u003d train_df.drop(\u0027ip_geopoint\u0027, axis\u003d1)\n#train_df[\u0027High Revenue\u0027] \u003d train_df[\u0027High Revenue\u0027].astype(\u0027string\u0027)\n\ntest \u003d dataiku.Dataset(\"customer_data_test_prepared\")\ntest_df \u003d test.get_dataframe()\n#test_df \u003d test_df.drop(\u0027ip_geopoint\u0027, axis\u003d1)\n#test_df[\u0027High Revenue\u0027] \u003d test_df[\u0027High Revenue\u0027].astype(\u0027string\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_df.dtypes"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Choose features to include in the model\ncolumns_to_inlcude \u003d [\u0027pages_visited\u0027, \u0027campaign\u0027, \u0027Country\u0027, \u0027GDP_per_cap\u0027, \u0027age\u0027, \u0027price_first_item_purchased\u0027, \u0027gender\u0027, \u0027High Revenue\u0027]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "columns_to_ignore \u003d [col for col in train_df.columns if col not in columns_to_inlcude]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# This section will sync the MLFlow experiments with Dataiku\nmlflow_model_cc_transaction_fraud_folder \u003d dataiku.Folder(XP_TRACKING_FOLDER_ID)\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()\n\nmlflow_extension \u003d project.get_mlflow_extension()\nmlflow_handle \u003d project.setup_mlflow(managed_folder\u003dmlflow_model_cc_transaction_fraud_folder)\n\nmlflow.set_experiment(experiment_name\u003dMLFLOW_EXPERIMENT_NAME)\nmlflow_experiment \u003d mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Name the run with current timestamp\ndef now_str() -\u003e str:\n    return datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n\nrun_name \u003d f\"{MODEL_NAME}_{now_str()}\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Catboost likes feature-type (categorical, numeric) indices\nnonint_features_indices \u003d np.where((train_df.dtypes !\u003d np.int))[0]\nnonfloat_features_indices \u003d np.where((train_df.dtypes !\u003d np.float))[0]\ncategorical_features_indices \u003d [value for value in nonint_features_indices if value in nonfloat_features_indices]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with mlflow.start_run(run_name\u003drun_name) as run:\n        run_id \u003d run.info.run_id\n\n        mlflow.set_tag(\"model\", \"catboost\")\n        mlflow.set_tag(\"stage\", \"experimenting\")\n        mlflow.set_tag(\"run_name\", run_name)\n\n        X \u003d train_df.drop(\u0027High Revenue\u0027, axis\u003d1)\n        y \u003d train_df[\u0027High Revenue\u0027]\n\n        nonint_features_indices \u003d np.where((X.dtypes !\u003d np.int))[0]\n        nonfloat_features_indices \u003d np.where((X.dtypes !\u003d np.float))[0]\n        categorical_features_indices \u003d [value for value in nonint_features_indices if value in nonfloat_features_indices]\n\n        X_train, X_test, y_train, y_test \u003d train_test_split(X, y, test_size\u003d0.2, random_state\u003d42)\n\n        train_data \u003d Pool(data\u003dX_train, label\u003dy_train, cat_features\u003dcategorical_features_indices)\n        test_data \u003d Pool(data\u003dX_test, label\u003dy_test, cat_features\u003dcategorical_features_indices)\n\n        # Hyperparameters space here\n        param \u003d {\u0027objective\u0027         : \"Logloss\",\n                 \"ignored_features\": columns_to_ignore,\n                 \u0027learning_rate\u0027 : 0.04\n                 }\n\n        # Use MLFlow to log chosen parameters\n        mlflow.log_params(param)\n\n\n        cat_cls \u003d CatBoostClassifier(**param)\n        cat_cls.fit(train_data, eval_set \u003d test_data, verbose\u003d0)\n        mlflow.catboost.log_model(cat_cls, artifact_path\u003df\"{run_name}\")\n        preds \u003d cat_cls.predict(X_test)\n        #convert_if \u003d lambda t: 0 if t \u003d\u003d\"low_revenue\" else 1\n        #preds_converted \u003d np.array([convert_if(predsi) for predsi in preds])\n        pred_labels \u003d preds.astype(\u0027bool\u0027)\n\n        roc_auc \u003d round(roc_auc_score(y_test, pred_labels),4)\n        #accuracy \u003d round(accuracy_score(y_test, pred_labels),4)\n\n        mlflow.log_metric(\"roc_auc\", roc_auc)\n        #mlflow.log_metric(\"accuracy\", accuracy)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get the MLFlow details of the final, best trained model\nexperiment_id \u003d mlflow_experiment.experiment_id\nexperiment_results_df \u003d mlflow.search_runs(experiment_id)\n\nlatest_run_results_df \u003d experiment_results_df[experiment_results_df[\u0027tags.run_name\u0027] \u003d\u003d run_name]\nbest_run_id \u003d latest_run_results_df.iloc[0][\u0027run_id\u0027]\nmodel_path \u003d f\"clv_mlflow_exp/{best_run_id}/artifacts/{run_name}\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_path"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a new Dataiku Saved Model (if doesn\u0027t exist already)\nsm_id \u003d None\nfor sm in project.list_saved_models():\n    if sm[\"name\"] !\u003d SAVED_MODEL_NAME:\n        continue\n    else:\n        sm_id \u003d sm[\"id\"]\n        print(f\"Found Saved Model {sm[\u0027name\u0027]} with id {sm[\u0027id\u0027]}\")\n        break\n\nif sm_id:\n    sb \u003d project.get_saved_model(sm_id)\nelse:\n    sm \u003d project.create_mlflow_pyfunc_model(name\u003dSAVED_MODEL_NAME,\n                                            prediction_type\u003dDSSPredictionMLTaskSettings.PredictionTypes.BINARY)\n    sm_id \u003d sm.id\n    print(f\"Saved Model not found, created new one with id {sm_id}\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MLFLOW_CODE_ENV_NAME"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import the final trained model into the Dataiku Saved Model (Green Diamond)\nmlflow_version \u003d sm.import_mlflow_version_from_managed_folder(version_id\u003drun_name,\n                                                              managed_folder\u003dXP_TRACKING_FOLDER_ID,\n                                                              path\u003dmodel_path,\n                                                              code_env_name\u003dMLFLOW_CODE_ENV_NAME)\n\n# Make this Saved Model version the active one\nsm.set_active_version(mlflow_version.version_id)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set model metadata (target name, classes,...)\nmlflow_version.set_core_metadata(\u0027High Revenue\u0027, [\u0027false\u0027, \u0027true\u0027] , get_features_from_dataset\u003dEVALUATION_DATASET)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate the performance of this new version, to populate the performance screens of the saved model version in DSS\n#mlflow_version.evaluate(EVALUATION_DATASET)\nmlflow_version.evaluate(\u0027customer_data_test_prepared\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}